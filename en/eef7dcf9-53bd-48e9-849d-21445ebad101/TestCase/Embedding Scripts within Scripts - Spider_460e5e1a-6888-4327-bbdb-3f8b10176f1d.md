{
  "id": "135",
  "name": "Embedding Scripts within Scripts - Spider",
  "description": "Using a browser or an automated tool, an attacker records all entry points for inputs that happen to be reflected in a client-side script element. These script elements can be located in the HTML content (head, body, comments), in an HTML tag, XML, CSS, etc.\nAttack Step Techniques\nID\tAttack Step Technique Description\tEnvironments\n1\t\nUse a spidering tool to follow and record all non-static links that are likely to have input parameters (through forms, URL, fragments, etc.) actively used by the Web application.\nenv-Web\n2\t\nUse a proxy tool to record all links visited during a manual traversal of the web application.\nenv-Web\n3\t\nUse a browser to manually explore the website and analyze how it is constructed. Many browsers' plugins are available to facilitate the analysis or automate the discovery.\nenv-Web\nIndicators\nID\tType\tIndicator Description\tEnvironments\n1\tPositive\t\nInputs are used in a script element (script tag, DOM, etc.) and not in another type of element.\nenv-Web\n2\tInconclusive\t\nUsing URL rewriting, parameters may be part of the URL path or the URL fragment.\nenv-Web\n3\tInconclusive\t\nNo parameters appear to be used on the current page. Even though none appear, the web application may still use them if they are provided.\nenv-Web\n4\tNegative\t\nApplications that have only static pages or that simply present information without accepting input are unlikely to be susceptible.\nenv-Web\nOutcomes\nID\tType\tOutcome Description\n1\tSuccess\t\nA list of URLs, with their corresponding parameters (POST, GET, COOKIE, etc.) is created by the attacker. These parameters are possibly used in client-side scripts elements.\n2\tSuccess\t\nA list of application user interface entry fields is created by the attacker.\n3\tSuccess\t\nA list of resources accessed by the application is created by the attacker.\nSecurity Controls\nID\tType\tSecurity Control Description\n1\tDetective\t\nMonitor velocity of page fetching in web logs. Humans who view a page and select a link from it will click far slower and far less regularly than tools. Tools make requests very quickly and the requests are typically spaced apart regularly (e.g. 0.8 seconds between them).\n2\tDetective\t\nCreate links on some pages that are visually hidden from web browsers. Using iframes, images, or other HTML techniques, the links can be hidden from web browsing humans, but visible to spiders and programs. A request for the page, then, becomes a good predictor of an automated tool probing the application.\n3\tPreventative\t\nUse CAPTCHA to prevent the use of the application by an automated tool.\n4\tPreventative\t\nActively monitor the application and either deny or redirect requests from origins that appear to be automated.",
  "labels": "CAPEC",
  "libraryId": "1",
  "guid": "460e5e1a-6888-4327-bbdb-3f8b10176f1d",
  "isHidden": false,
  "isReadOnlyLibraryEntity": false,
  "libraryGuid": "eef7dcf9-53bd-48e9-849d-21445ebad101"
}